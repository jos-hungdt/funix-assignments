{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["nC-i7jFLu9wr","odSGXXRkvH5x","8It0NqKMO4th","4BbLk8gINwT4"],"mount_file_id":"1LQMrFdBOyZPunafIQgCnWEFXxqrJz2e3","authorship_tag":"ABX9TyNmbJva26asGGCypowvC+O5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Mouting Google Drive to Colab"],"metadata":{"id":"nC-i7jFLu9wr"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dp94_4gjuzX2","executionInfo":{"status":"ok","timestamp":1719923888902,"user_tz":-420,"elapsed":20017,"user":{"displayName":"Hung Duong Thanh","userId":"17863479808442794134"}},"outputId":"b6f3635e-4a58-43d5-8983-89a7c0f945f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Install pyspark to runtime\n"],"metadata":{"id":"odSGXXRkvH5x"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgJ0cyQSu8Ux","executionInfo":{"status":"ok","timestamp":1720088741858,"user_tz":-420,"elapsed":60397,"user":{"displayName":"Hung Duong Thanh","userId":"17863479808442794134"}},"outputId":"3c8dee97-24b0-45bd-c7a0-ab733780ab43"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=3ade331cc11140a919be97b25b07ed25c89e1302de1f00eea472f47490d6a7af\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n"]}]},{"cell_type":"markdown","source":["## Final source code"],"metadata":{"id":"8It0NqKMO4th"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from collections import Counter\n","from pyspark.mllib.feature import Word2Vec, Word2VecModel\n","from pyspark.mllib.linalg import Vectors\n","from pyspark.mllib.tree import RandomForest, RandomForestModel\n","from pyspark.mllib.regression import LabeledPoint\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","\n","import os\n","import numpy as np\n","import re\n","\n","dataDirPath = '/content/drive/MyDrive/ColabDrive/20news-18828'\n","word2VecPath = '/content/drive/MyDrive/ColabDrive/models/word2VecModel/word2Vec'\n","\n","# Replace value of numTrees, maxDepth, maxBins to find the best hyperparameters\n","params = {\n","  'numTrees': 80,\n","  'maxDepth': 10,\n","  'maxBins': 64,\n","  'outputPath': '/content/drive/MyDrive/ColabDrive/doc-classifier/submission_final',\n","  'docClassifierModelPath': '/content/drive/MyDrive/ColabDrive/models/document_classifier_model'\n","}\n","\n","# ==============================================================================\n","def transformAndSplitData(dataDir):\n","  '''\n","    Transform and split documents\n","    return a tuple (<training data files>: array, <validation data files>: array, <testing data files>: array)\n","  '''\n","  # extract folder name (label) from directory path\n","  folderName = dataDir[0].split('/')[-1]\n","\n","  # sort the data files by name\n","  dataDir[2].sort(key=lambda x: int(x))\n","\n","  # each file should be converted to (\"<label>, <file name>\", <file path>)\n","  dataDict = []\n","  for fileName in dataDir[2]:\n","    path = dataDir[0] + \"/\" + fileName\n","    dataDict.append((folderName + \", \" + fileName, path))\n","\n","  # calculate and split data files into 3 sets for training, testing, and validation\n","  totalCount = len(dataDir[2])\n","  trainingCount = int(totalCount * 0.6)\n","  valCount = int(totalCount * 0.2)\n","\n","  trainingArr, valArr, testArr = np.split(dataDict, [trainingCount, trainingCount + valCount])\n","\n","  return (trainingArr, valArr, testArr)\n","\n","# ==============================================================================\n","def processFile(path):\n","  '''\n","    Read content of a file.\n","  '''\n","  with open(path, 'r', encoding=\"ISO-8859-1\") as file:\n","    content = file.read()\n","  return content\n","\n","# ==============================================================================\n","def removeSpecialChars(s):\n","  '''\n","    Remove special characters from document\n","  '''\n","  return re.sub(r'\\W+', ' ', s)\n","\n","# ==============================================================================\n","def generateBigrams(document):\n","  '''\n","    Function to generate bigrams from a document.\n","  '''\n","  words = document.split()\n","  bigrams = []\n","  for i in range(len(words) - 1):\n","    bigrams.append(words[i] + ' ' + words[i + 1])\n","  return bigrams\n","\n","# ==============================================================================\n","def parse_doc_to_labeledpoint(data):\n","  label, vec = data\n","  folder, filename = label.split(',')\n","\n","  # Ensure the vector is converted to a list with a fixed length\n","  features = vec.toArray().tolist()\n","\n","  # Pad or truncate the feature list to a fixed length if necessary\n","  fixed_length = 400\n","  features = features[:fixed_length] + [0] * (fixed_length - len(features))\n","  return LabeledPoint(folder_to_label[folder], features)\n","\n","# ==============================================================================\n","def transformToVector(document, broadcast_word_vectors):\n","  '''\n","    Function transform document to vector.\n","  '''\n","  docWords = document.split(\" \")\n","  bigrams = generateBigrams(document)\n","\n","  # create the first vector - word vector\n","  wordVec = np.asarray([1 if word in docWords else 0 for word in topMostAppearanceWords])\n","\n","  # create the second vector - bigram vector\n","  bigramVec = np.asarray([1 if bigram in bigrams else 0 for bigram in topBigramMostAppearance])\n","\n","  # combine 2 vectors\n","  combined_bi_word_vecs = np.append(wordVec, bigramVec)\n","\n","  # create word2Vec vector\n","  most_common_words = Counter(docWords).most_common(100)\n","  wordsVectors = broadcast_word_vectors.value\n","\n","  vectors = [wordsVectors[word] for word, _ in most_common_words if word in wordsVectors]\n","  avg_vector = np.mean(vectors, axis=0)\n","\n","  combined_word_vec = np.append(combined_bi_word_vecs, np.asarray(avg_vector))\n","  return Vectors.dense(combined_word_vec)\n","\n","# ==============================================================================\n","def process_document(doc_id, doc_content, broadcast_word_vectors):\n","  '''\n","    In case of using 200 words and 100 bigrams most appearance,\n","    use the following code line marked as #1, this will be used by default as assignment requirement.\n","\n","    The #2 is is alternative method to convert documents to vector,\n","    this method will use the word2Vec model only for the process.\n","\n","    Uncomment the #1 code line or the #2 code line depend on which method you want to use.\n","    NOTE: #1 is much better than #2 after testing.\n","  '''\n","  #1\n","  vector = transformToVector(doc_content.lower(), broadcast_word_vectors)\n","\n","  #2\n","  # vector = doc_to_word2vec(doc_content.lower(), broadcast_word_vectors)\n","  return (doc_id, vector)\n","\n","# ==============================================================================\n","def doc_to_word2vec(data, broadcast_word_vectors):\n","  '''\n","    Transform document to vector 400 length, only use word2vec model.\n","    - Find 400 most common words in the document\n","    - Use word2vec model to get the vector of each word and calculate the average vector of the words.\n","  '''\n","  doc_words = data.split(\" \")\n","  # create word2Vec vector\n","  most_common_words = Counter(doc_words).most_common(400)\n","  word_vectors = broadcast_word_vectors.value\n","\n","  vectors = [Vectors.dense(word_vectors[word]) for word, _ in most_common_words if word in word_vectors]\n","\n","  if vectors:\n","    avg_vector = sum(vectors) / len(vectors)\n","    return avg_vector\n","  else:\n","    return Vectors.dense([0.0] * 400)\n","\n","# ==============================================================================\n","# >>> Main program\n","# ==============================================================================\n","print(\">>> Initialize spark session. Parameters will be using: \\n\" \\\n","      f\"numTrees: {params['numTrees']}\\n\" \\\n","      f\"maxDepth: {params['maxDepth']}\\n\" \\\n","      f\"maxBins: {params['maxBins']}\\n\" \\\n","      f\"output files will be exported at: {params['outputPath']}\\n\" \\\n","      f\"Trained model will be saved to (if available): {params['docClassifierModelPath']}\\n\" \\\n","      \"------------------------------------------------------------------------- \\n\"\n",")\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"document_classifier\") \\\n","    .getOrCreate()\n","sc = spark.sparkContext\n","dataDirs = [x for x in os.walk(dataDirPath)]\n","dataDirs.sort(key=lambda x: x[0])\n","\n","# load all data folders and for each of folder, get all file names and split into 3 arrays\n","trainingArr = []\n","valArr = []\n","testArr = []\n","\n","#*****************************************\n","print('ASM-3: read and separate data set;')\n","#*****************************************\n","\n","for dataDir in dataDirs:\n","  if len(dataDir[2]) < 1:\n","    continue\n","  trainingDataDirs, valDataDirs, testDataDirs = transformAndSplitData(dataDir)\n","\n","  trainRDD = sc.parallelize(trainingDataDirs).map(lambda x: (x[0], processFile(x[1])))\n","  valRDD = sc.parallelize(valDataDirs).map(lambda x: (x[0], processFile(x[1])))\n","  testRDD = sc.parallelize(testDataDirs).map(lambda x: (x[0], processFile(x[1])))\n","\n","  trainingArr.append(trainRDD)\n","  valArr.append(valRDD)\n","  testArr.append(testRDD)\n","\n","#*****************************************\n","print('ASM-3: union sets of RDD and cache to memory. \\n')\n","#*****************************************\n","trainRDDs = sc.union(trainingArr)\n","valRDDs = sc.union(valArr)\n","testRDDs = sc.union(testArr)\n","\n","trainRDDs.cache()\n","valRDDs.cache()\n","testRDDs.cache()\n","\n","#*****************************************\n","print('ASM-4: converts all words to lower case and remove special characters;')\n","#*****************************************\n","# converts all words to lower case and remove special characters because they are not a word.\n","lowerCaseRdd = trainRDDs.map(lambda x: removeSpecialChars(x[1].lower()))\n","\n","#*****************************************\n","print('ASM-4: find top 200 words most appearance;')\n","#*****************************************\n","excludedWords = ['in','on','of','out','by','from','to','over','under','the','a','an','when','where','what','who','whom','you','thou','go', \\\n","                 'must','i','me','my','myself','for','and','x','it','are', '0','1','2','3','4','5','6','7','8','9','be','thi','with','this', \\\n","                 'that','or','if','have','t','an','db','but','at','wa','they','will','can','a','b','c','d','e','f','g','h','i','j','k','l', \\\n","                 'm','n', 'o','p','q','r','s','t','u','v','w','x','y','z','one','zero','one','two','three','four','five','six','seven', \\\n","                 'eight','nine','ten','do','did','here','there','all','subject','about','we','other', 'no','re','ha','which','your','so', \\\n","                 'would','some','their','he','any','more','how','only','may','might','also','new','should','up','hi','dear','them','then', \\\n","                 'first','second','third','don','doe', 'were','know','than','less','most','get','year','like','been','use','many', 'few', \\\n","                 'little','just','make','these','those','because','not','into']\n","\n","# split each document to arrays of words, filter out the excluded words, then create frequency tuple\n","wordsRdd = lowerCaseRdd.flatMap(lambda x: x.split(' ')) \\\n","  .filter(lambda x: x not in excludedWords and x) \\\n","  .map(lambda x: (x, 1))\n","\n","frequencyWordRdd = wordsRdd.reduceByKey(lambda a, b: a + b)\n","topMostAppearanceWords = frequencyWordRdd.top(200, key=lambda x: x[1])\n","# print(\"2. Top 200 words most appearance = \", topMostAppearanceWords)\n","print('ASM-4: complete finding most appearance words;')\n","\n","#*****************************************\n","print('ASM-4: find top 100 bigram most appearance;')\n","#*****************************************\n","excludedBigrams = ['of the','x x','in the','to the','it i','on the','to be','for the','i a','subject re','and the','if you','don t','that the', \\\n","                   'in article','0 1','1 1','from the','thi i','with the','i not','it ','i the','the same','in a','of a','that i','for a','by the', \\\n","                   'will be','i m','i have','there i','the first','you are','with a','n x','a a','what i','doe not','to a','at the','do not', \\\n","                   'would be','can be','there are','1 0','i am','they are','are not','you can','on a','and i','should be','may be','and a','have a', \\\n","                   'have been','such a','number of','that you','i ve','about the',' want to','that it','which i','the following','x printf','but i', \\\n","                   'i don','can t','x if','file x','to do','to get','you have','one of','and other','a the','doesn t','1 2','i can','that they', \\\n","                   'out of','i to','i that','all the','the other','how to','of thi','into the','be a','to have','c si','i think','are the','to make', \\\n","                   'ha been','isn t','0 0','x char','ha a','must be','mov bh','it wa','have to','in thi','for example','if the',' a','not a','that ', \\\n","                   'x the','of course','at least','a good','you re','write in','not to','part of','i one','2 2','your entry','but the','a few','the u', \\\n","                   'the only','i would','i an','a well','u ','and that','i wa','sort of','lot of','0 2','but it','if i','the most','and at','all of', \\\n","                   'to use','seem to','and it','i know','bl bh','to see','want to']\n","\n","bigramRdd = lowerCaseRdd.flatMap(lambda d: generateBigrams(d))\n","filteredBigramRdd = bigramRdd.filter(lambda x: x and x not in excludedBigrams) \\\n","  .map(lambda x: (x, 1))\n","\n","frequencyBigramRdd = filteredBigramRdd.reduceByKey(lambda a, b: a + b)\n","topBigramMostAppearance = frequencyBigramRdd.top(100, key=lambda x: x[1])\n","# print(\"3. Top 100 bigrams most appearance = \", topBigramMostAppearance)\n","print('ASM-4: complete finding most appearance bigrams. \\n')\n","\n","#*****************************************\n","print('ASM-5: train Word2Vec by documents in train RDD;')\n","#*****************************************\n","word2vec = Word2Vec()\n","inp = lowerCaseRdd.map(lambda x: x.split(\" \"))\n","inp.cache()\n","word2VecModel = word2vec.fit(inp)\n","\n","#*****************************************\n","print('ASM-5: save model for the next use;')\n","#*****************************************\n","word2VecModel.save(sc, word2VecPath)\n","\n","# Training is expensive! Load the trained model if it's exist by uncomment the code line below.\n","# word2VecModel = Word2VecModel.load(sc, word2VecPath)\n","\n","#*****************************************\n","print('ASM-5: broadcast word vectors for transform document to vector process;')\n","#*****************************************\n","wordVectors = word2VecModel.getVectors()\n","wordVectorsDict = {word: list(vector) for word, vector in wordVectors.items()}\n","broadcastWordVectors = sc.broadcast(wordVectorsDict)\n","\n","#*****************************************\n","print('ASM-5: transforms documents to vectors. \\n')\n","#*****************************************\n","trainVectorRdd = trainRDDs.map(lambda x: process_document(x[0], x[1], broadcastWordVectors))\n","trainVectorRdd.cache()\n","\n","valVectorRdd = valRDDs.map(lambda x: process_document(x[0], x[1], broadcastWordVectors))\n","valVectorRdd.cache()\n","\n","testVectorRdd = testRDDs.map(lambda x: process_document(x[0], x[1], broadcastWordVectors))\n","testVectorRdd.cache()\n","\n","#*****************************************\n","print('ASM-6: convert documents to LabeledPoints;')\n","#*****************************************\n","trainLabeledPointRdd = trainVectorRdd.map(parse_doc_to_labeledpoint)\n","valLabeledPointRdd = valVectorRdd.map(parse_doc_to_labeledpoint)\n","testLabeledPointRdd = testVectorRdd.map(parse_doc_to_labeledpoint)\n","\n","#*****************************************\n","print('ASM-6: create a dictionary to map folder names to labels and calculate numClasses for the training;')\n","#*****************************************\n","folder_names = [dataDir[0].split('/')[-1] for dataDir in dataDirs]\n","folder_names.pop(0)\n","folder_to_label = {name: i for i, name in enumerate(folder_names)}\n","numClasses = len(folder_names) + 1\n","\n","#*****************************************\n","print('ASM-6: training Random Forest classifier model;')\n","#*****************************************\n","classifier_model = RandomForest.trainClassifier(trainLabeledPointRdd, \\\n","                                                numClasses=numClasses, \\\n","                                                categoricalFeaturesInfo={}, \\\n","                                                numTrees=params['numTrees'], \\\n","                                                featureSubsetStrategy=\"auto\", \\\n","                                                impurity='gini', \\\n","                                                maxDepth=params['maxDepth'], \\\n","                                                maxBins=params['maxBins'])\n","\n","#*****************************************\n","print('ASM-6: complete training Random Forest classifier model, saving for the next use;')\n","#*****************************************\n","# training is expensive, save for later!\n","classifier_model.save(sc, params['docClassifierModelPath'])\n","\n","# The below code line is for loading the saved model if exist. This will save time for testing.\n","classifier_model = RandomForestModel.load(sc, params['docClassifierModelPath'])\n","\n","#*****************************************\n","print('ASM-6: start evaluating the model on validation data;')\n","#*****************************************\n","predictions = classifier_model.predict(valLabeledPointRdd.map(lambda x: x.features))\n","\n","validLabelRdd = valLabeledPointRdd.map(lambda x: x.label)\n","valid_prediction_and_labels = validLabelRdd.zip(predictions)\n","valid_multiclass_metrics = MulticlassMetrics(valid_prediction_and_labels)\n","\n","print('ASM-6: validation data evaluation results:')\n","print(\"-> Accuracy:\", valid_multiclass_metrics.accuracy)\n","print(\"-> Weighted Precision:\", valid_multiclass_metrics.weightedPrecision)\n","print(\"-> Weighted Recall:\", valid_multiclass_metrics.weightedRecall)\n","print(\"-> Weighted F1 Score:\", valid_multiclass_metrics.weightedFMeasure())\n","print(\"-> Confusion Matrix:\\n\", valid_multiclass_metrics.confusionMatrix().toArray())\n","\n","#*****************************************\n","print('ASM-6: Run model with test data and write to output file.')\n","#*****************************************\n","test_predictions = classifier_model.predict(testLabeledPointRdd.map(lambda x: x.features))\n","test_numeric_labels = testLabeledPointRdd.map(lambda x: int(x.label))\n","test_prediction_and_numeric_labels = test_numeric_labels.zip(test_predictions)\n","test_prediction_and_labels = testRDDs.map(lambda x: x[0]).zip(test_predictions.map(lambda x: folder_names[int(x)]))\n","\n","# Convert to DataFrame with 3 columns Origin folder, document name and the model prediction\n","results = spark.createDataFrame(test_prediction_and_labels.map(lambda x: (x[0].split(',')[0], x[0].split(',')[1], x[1]))).toDF(\"origin-folder\", \"doc-name\", \"prediction\")\n","\n","# Show DataFrame to output for quick checking.\n","results.show()\n","\n","# Save results to CSV file(s).\n","# Using coalesce(1) help to merge all partitions into a single DataFrame and so, only 1 CSV file will be created with all data. BUT it's expensive to do so.\n","# For testing, remove the .coalsesce(1) and just left the Spark decided by itself.\n","results.coalesce(1).write.csv(params['outputPath'], header=True)\n","\n","#*****************************************\n","print('ASM-6: Completed writing output file. Check it out.')\n","#*****************************************\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aW3anaAnO4aX","outputId":"ab61d6c7-a232-4ea8-b9b4-7c58b72668fe","executionInfo":{"status":"ok","timestamp":1720087138549,"user_tz":-420,"elapsed":2680824,"user":{"displayName":"Hung Duong Thanh","userId":"17863479808442794134"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Initialize spark session. Parameters will be using: \n","numTrees: 80\n","maxDepth: 10\n","maxBins: 64\n","output files will be exported at: /content/drive/MyDrive/ColabDrive/doc-classifier/submission_final\n","Trained model will be saved to (if available): /content/drive/MyDrive/ColabDrive/models/document_classifier_model\n","------------------------------------------------------------------------- \n","\n","ASM-3: read and separate data set;\n","ASM-3: union sets of RDD and cache to memory. \n","\n","ASM-4: converts all words to lower case and remove special characters;\n","ASM-4: find top 200 words most appearance;\n","ASM-4: complete finding most appearance words;\n","ASM-4: find top 100 bigram most appearance;\n","ASM-4: complete finding most appearance bigrams. \n","\n","ASM-5: train Word2Vec by documents in train RDD;\n","ASM-5: save model for the next use;\n","ASM-5: broadcast word vectors for transform document to vector process;\n","ASM-5: transforms documents to vectors. \n","\n","ASM-6: convert documents to LabeledPoints;\n","ASM-6: create a dictionary to map folder names to labels and calculate numClasses for the training;\n","ASM-6: training Random Forest classifier model;\n","ASM-6: complete training Random Forest classifier model, saving for the next use;\n","ASM-6: start evaluating the model on validation data;\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["ASM-6: validation data evaluation results:\n","-> Accuracy: 0.4639531790369779\n","-> Weighted Precision: 0.5125848579870775\n","-> Weighted Recall: 0.4639531790369779\n","-> Weighted F1 Score: 0.47519072361610004\n","-> Confusion Matrix:\n"," [[ 58.   1.   1.   0.   1.   0.   0.   1.   0.   0.   1.   1.   0.   2.\n","    1.  22.   0.   6.   5.  10.]\n"," [  0.  97.  30.  13.  18.  34.   9.   2.   1.   3.   0.   6.  19.  13.\n","    8.   2.   0.   2.   2.   1.]\n"," [  1.  16.  69.  19.   6.  12.   6.   3.   0.   1.   0.   0.  15.   2.\n","    4.   0.   0.   0.   0.   1.]\n"," [  0.   3.  24.  84.  29.   0.  14.   0.   0.   0.   0.   0.  11.   0.\n","    0.   0.   0.   1.   0.   0.]\n"," [  0.   6.  10.  30.  75.   0.  10.   6.   0.   0.   0.   2.  13.   2.\n","    0.   0.   0.   0.   0.   0.]\n"," [  0.  21.  22.   3.   2. 110.   1.   0.   1.   0.   0.   1.   5.   0.\n","    1.   0.   0.   0.   0.   0.]\n"," [  0.   5.   3.   9.  15.   5. 109.   7.   9.   1.   2.   1.  10.   4.\n","    1.   0.   1.   0.   1.   0.]\n"," [  0.   0.   3.   3.   4.   1.  10.  89.  33.   5.   4.   1.   9.   2.\n","    0.   0.   3.   2.   3.   2.]\n"," [  1.   2.   0.   4.   4.   5.   9.  45. 107.  13.  11.   0.   6.   9.\n","    5.   0.   5.   2.   3.   3.]\n"," [  1.   1.   0.   0.   1.   0.   2.   4.   8. 108.  54.   0.   1.   3.\n","    1.   1.   3.   2.   4.   2.]\n"," [  1.   0.   0.   0.   0.   0.   5.   1.   1.  48. 118.   0.   0.   0.\n","    0.   0.   1.   1.   2.   0.]\n"," [  0.   4.   2.   1.   2.   4.   1.   0.   0.   0.   0. 133.  12.   2.\n","    2.   0.  11.   4.   3.   0.]\n"," [  0.  10.   6.  13.  11.   4.   7.   6.   0.   0.   0.   2.  51.   2.\n","    5.   0.   0.   0.   2.   2.]\n"," [ 15.   9.  12.  13.   9.  12.   1.  15.  10.   2.   1.  17.  23. 105.\n","   47.  25.  18.  11.  20.  15.]\n"," [  5.  12.  14.   3.  13.   6.   4.   6.   4.   2.   3.   6.  14.  20.\n","  107.   2.  12.   9.   8.   5.]\n"," [ 60.   5.   0.   1.   1.   1.   1.   4.  12.   8.   0.   6.   3.  18.\n","    6. 123.  14.  24.  19.  59.]\n"," [  5.   1.   0.   0.   0.   0.   2.   7.   9.   6.   5.  17.   3.   6.\n","    1.   5.  84.  15.  32.  12.]\n"," [ 11.   0.   1.   0.   0.   1.   3.   1.   1.   1.   0.   4.   1.   6.\n","    7.  19.  28. 108.  44.   9.]\n"," [  1.   1.   0.   0.   1.   1.   0.   1.   2.   0.   0.   1.   0.   2.\n","    1.   0.   2.   1.   7.   2.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","    0.   0.   0.   0.   0.   2.]]\n","ASM-6: Run model with test data and write to output file.\n","+-------------+--------+--------------------+\n","|origin-folder|doc-name|          prediction|\n","+-------------+--------+--------------------+\n","|  alt.atheism|   53658|soc.religion.chri...|\n","|  alt.atheism|   53660|soc.religion.chri...|\n","|  alt.atheism|   53661|soc.religion.chri...|\n","|  alt.atheism|   53663|soc.religion.chri...|\n","|  alt.atheism|   53664|soc.religion.chri...|\n","|  alt.atheism|   53667|         alt.atheism|\n","|  alt.atheism|   53669|soc.religion.chri...|\n","|  alt.atheism|   53670|         alt.atheism|\n","|  alt.atheism|   53671|         alt.atheism|\n","|  alt.atheism|   53673|             sci.med|\n","|  alt.atheism|   53675|  talk.politics.guns|\n","|  alt.atheism|   53676|         alt.atheism|\n","|  alt.atheism|   53677|soc.religion.chri...|\n","|  alt.atheism|   53753|talk.politics.mid...|\n","|  alt.atheism|   53754|         alt.atheism|\n","|  alt.atheism|   53755|         alt.atheism|\n","|  alt.atheism|   53756|soc.religion.chri...|\n","|  alt.atheism|   53757|             sci.med|\n","|  alt.atheism|   53758|         alt.atheism|\n","|  alt.atheism|   53759|  talk.politics.guns|\n","+-------------+--------+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"markdown","source":["## Try to change vectors"],"metadata":{"id":"4BbLk8gINwT4"}},{"cell_type":"code","source":["params = {\n","  'numTrees': 80,\n","  'maxDepth': 10,\n","  'maxBins': 64,\n","  'outputPath': '/content/drive/MyDrive/ColabDrive/doc-classifier/submission_80_10_64',\n","  'docClassifierModelPath': '/content/drive/MyDrive/ColabDrive/models/docClassifierModel_80_10_64'\n","}\n","# ==============================================================================\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"document_classifier\") \\\n","    .getOrCreate()\n","sc = spark.sparkContext\n","dataDirs = [x for x in os.walk(dataDirPath)]\n","dataDirs.sort(key=lambda x: x[0])\n","\n","# load all data folders and for each of folder, get all file names and split into 3 arrays\n","trainingArr = []\n","valArr = []\n","testArr = []\n","\n","#*****************************************\n","print('ASM3 - read and separate data set')\n","#*****************************************\n","\n","for dataDir in dataDirs:\n","  if len(dataDir[2]) < 1:\n","    continue\n","  trainingDataDirs, valDataDirs, testDataDirs = transformAndSplitData(dataDir)\n","\n","  trainRDD = sc.parallelize(trainingDataDirs).map(lambda x: (x[0], processFile(x[1])))\n","  valRDD = sc.parallelize(valDataDirs).map(lambda x: (x[0], processFile(x[1])))\n","  testRDD = sc.parallelize(testDataDirs).map(lambda x: (x[0], processFile(x[1])))\n","\n","  trainingArr.append(trainRDD)\n","  valArr.append(valRDD)\n","  testArr.append(testRDD)\n","\n","#*****************************************\n","print('ASM-3: union sets of RDD and cache to memory. \\n')\n","#*****************************************\n","trainRDDs = sc.union(trainingArr)\n","valRDDs = sc.union(valArr)\n","testRDDs = sc.union(testArr)\n","\n","trainRDDs.cache()\n","valRDDs.cache()\n","testRDDs.cache()\n","\n","#*****************************************\n","print('ASM-4: converts all words to lower case and remove special characters.')\n","#*****************************************\n","# converts all words to lower case and remove special characters because they are not a word.\n","lowerCaseRdd = trainRDDs.map(lambda x: removeSpecialChars(x[1].lower()))\n","\n","#*****************************************\n","print('ASM-4: find top 200 words most appearance')\n","#*****************************************\n","excludedWords = ['in','on','of','out','by','from','to','over','under','the','a','an','when','where','what','who','whom','you','thou','go','must','i','me','my','myself','for','and','x','it','are', '0','1','2','3','4','5','6','7','8','9','be','thi','with','this','that','or','if','have','t','an','db','but','at','wa','they','will','can','a','b','c','d','e','f','g','h','i','j','k','l','m','n', 'o','p','q','r','s','t','u','v','w','x','y','z','one','zero','one','two','three','four','five','six','seven','eight','nine','ten','do','did','here','there','all','subject','about','we','other', 'no','re','ha','which','your','so','would','some','their','he','any','more','how','only','may','might','also','new','should','up','hi','dear','them','then','first','second','third','don','doe', 'were','know','than','less','most','get','year','like','been','use','many', 'few','little','just','make','these','those','because','not','into']\n","\n","# split each document to arrays of words, filter out the excluded words, then create frequency tuple\n","wordsRdd = lowerCaseRdd.flatMap(lambda x: x.split(' ')) \\\n","  .filter(lambda x: x not in excludedWords and x) \\\n","  .map(lambda x: (x, 1))\n","\n","frequencyWordRdd = wordsRdd.reduceByKey(lambda a, b: a + b)\n","topMostAppearanceWords = frequencyWordRdd.top(200, key=lambda x: x[1])\n","# print(\"2. Top 200 words most appearance = \", topMostAppearanceWords)\n","print('ASM-4: complete finding most appearance words.')\n","\n","#*****************************************\n","print('ASM-4: find top 100 bigram most appearance')\n","#*****************************************\n","excludedBigrams = ['of the','x x','in the','to the','it i','on the','to be','for the','i a','subject re','and the','if you','don t','that the','in article','0 1','1 1','from the','thi i','with the','i not','it ','i the','the same','in a','of a','that i','for a','by the','will be','i m','i have','there i','the first','you are','with a','n x','a a','what i','doe not','to a','at the','do not','would be','can be','there are','1 0','i am','they are','are not','you can','on a','and i','should be','may be','and a','have a','have been','such a','number of','that you','i ve','about the',' want to','that it','which i','the following','x printf','but i','i don','can t','x if','file x','to do','to get','you have','one of','and other','a the','doesn t','1 2','i can','that they','out of','i to','i that','all the','the other','how to','of thi','into the','be a','to have','c si','i think','are the','to make','ha been','isn t','0 0','x char','ha a','must be','mov bh','it wa','have to','in thi','for example','if the',' a','not a','that ','x the','of course','at least','a good','you re','write in','not to','part of','i one','2 2','your entry','but the','a few','the u','the only','i would','i an','a well','u ','and that','i wa','sort of','lot of','0 2','but it','if i','the most','and at','all of','to use','seem to','and it','i know','bl bh','to see','want to']\n","\n","bigramRdd = lowerCaseRdd.flatMap(lambda d: generateBigrams(d))\n","filteredBigramRdd = bigramRdd.filter(lambda x: x and x not in excludedBigrams) \\\n","  .map(lambda x: (x, 1))\n","\n","frequencyBigramRdd = filteredBigramRdd.reduceByKey(lambda a, b: a + b)\n","topBigramMostAppearance = frequencyBigramRdd.top(100, key=lambda x: x[1])\n","# print(\"3. Top 100 bigrams most appearance = \", topBigramMostAppearance)\n","print('ASM-4: complete finding most appearance bigrams. \\n')\n","\n","#*****************************************\n","print('ASM-5: train Word2Vec by documents in train RDD')\n","#*****************************************\n","# word2vec = Word2Vec()\n","# inp = lowerCaseRdd.map(lambda x: x.split(\" \"))\n","# inp.cache()\n","# word2VecModel = word2vec.fit(inp)\n","\n","#*****************************************\n","print('ASM-5: save model for the next use.')\n","#*****************************************\n","# word2VecModel.save(sc, word2VecPath)\n","\n","# Load the trained model\n","word2VecModel = Word2VecModel.load(sc, word2VecPath)\n","\n","#*****************************************\n","print('ASM-5: broadcast word vectors for transform document to vector process.')\n","#*****************************************\n","wordVectors = word2VecModel.getVectors()\n","wordVectorsDict = {word: list(vector) for word, vector in wordVectors.items()}\n","broadcastWordVectors = sc.broadcast(wordVectorsDict)\n","\n","#*****************************************\n","print('ASM-5: transforms documents to vectors. \\n')\n","#*****************************************\n","trainVectorRdd = trainRDDs.map(lambda x: processDocument(x[0], x[1], broadcastWordVectors))\n","trainVectorRdd.cache()\n","\n","valVectorRdd = valRDDs.map(lambda x: processDocument(x[0], x[1], broadcastWordVectors))\n","valVectorRdd.cache()\n","\n","testVectorRdd = testRDDs.map(lambda x: processDocument(x[0], x[1], broadcastWordVectors))\n","testVectorRdd.cache()\n","\n","trainLabeledPointRdd = trainVectorRdd.map(parse_doc_to_labeledpoint)\n","valLabeledPointRdd = valVectorRdd.map(parse_doc_to_labeledpoint)\n","testLabeledPointRdd = testVectorRdd.map(parse_doc_to_labeledpoint)\n","\n","# Create a dictionary to map folder names to labels\n","folder_names = [dataDir[0].split('/')[-1] for dataDir in dataDirs]\n","folder_names.pop(0)\n","folder_to_label = {name: i for i, name in enumerate(folder_names)}\n","\n","#*****************************************\n","print('ASM-6: training Random Forest classifier model')\n","#*****************************************\n","# numClasses = len(folder_names) + 1\n","# classifier_model = RandomForest.trainClassifier(trainLabeledPointRdd, \\\n","#                                                 numClasses=numClasses, \\\n","#                                                 categoricalFeaturesInfo={}, \\\n","#                                                 numTrees=params['numTrees'], \\\n","#                                                 featureSubsetStrategy=\"auto\", \\\n","#                                                 impurity='gini', \\\n","#                                                 maxDepth=params['maxDepth'], \\\n","#                                                 maxBins=params['maxBins'])\n","\n","#*****************************************\n","print('ASM-6: save model for the next use')\n","#*****************************************\n","# classifier_model.save(sc, params['docClassifierModelPath'])\n","\n","classifier_model = RandomForestModel.load(sc, params['docClassifierModelPath'])\n","\n","#*****************************************\n","print('ASM-6: completed training model, start evaluate model.')\n","#*****************************************\n","predictions = classifier_model.predict(valLabeledPointRdd.map(lambda x: x.features))\n","\n","validLabelRdd = valLabeledPointRdd.map(lambda x: x.label)\n","valid_prediction_and_labels = validLabelRdd.zip(predictions)\n","valid_multiclass_metrics = MulticlassMetrics(valid_prediction_and_labels)\n","\n","print('ASM-6: validation data evaluation results:')\n","print(\"-> Accuracy:\", valid_multiclass_metrics.accuracy)\n","print(\"-> Weighted Precision:\", valid_multiclass_metrics.weightedPrecision)\n","print(\"-> Weighted Recall:\", valid_multiclass_metrics.weightedRecall)\n","print(\"-> Weighted F1 Score:\", valid_multiclass_metrics.weightedFMeasure())\n","print(\"-> Confusion Matrix:\\n\", valid_multiclass_metrics.confusionMatrix().toArray())\n","\n","#*****************************************\n","print('ASM-6: Run model with test data and write to output file.')\n","#*****************************************\n","test_predictions = classifier_model.predict(testLabeledPointRdd.map(lambda x: x.features))\n","test_numeric_labels = testLabeledPointRdd.map(lambda x: int(x.label))\n","test_prediction_and_numeric_labels = test_numeric_labels.zip(test_predictions)\n","test_prediction_and_labels = testRDDs.map(lambda x: x[0]).zip(test_predictions.map(lambda x: folder_names[int(x)]))\n","\n","# Convert to DataFrame\n","results = spark.createDataFrame(test_prediction_and_labels.map(lambda x: (x[0].split(',')[0], x[0].split(',')[1], x[1]))).toDF(\"origin-folder-name\", \"doc-name\", \"prediction\")\n","\n","# Show DataFrame\n","results.show()\n","\n","# Save results to CSV\n","results.write.csv(params['outputPath'], header=True)\n","\n","spark.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d89EShoxNzwf","executionInfo":{"status":"ok","timestamp":1720074733367,"user_tz":-420,"elapsed":1753971,"user":{"displayName":"Hung Duong Thanh","userId":"17863479808442794134"}},"outputId":"be142ff5-f23b-493b-d176-63804c623fea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ASM3 - read and separate data set\n","ASM-3: union sets of RDD and cache to memory. \n","\n","ASM-4: converts all words to lower case and remove special characters.\n","ASM-4: find top 200 words most appearance\n","ASM-4: complete finding most appearance words.\n","ASM-4: find top 100 bigram most appearance\n","ASM-4: complete finding most appearance bigrams. \n","\n","ASM-5: train Word2Vec by documents in train RDD\n","ASM-5: save model for the next use.\n","ASM-5: broadcast word vectors for transform document to vector process.\n","ASM-5: transforms documents to vectors. \n","\n","ASM-6: training Random Forest classifier model\n","ASM-6: save model for the next use\n","ASM-6: completed training model, start evaluate model.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["ASM-6: validation data evaluation results:\n","-> Accuracy: 0.0521415270018622\n","-> Weighted Precision: 1.0\n","-> Weighted Recall: 0.0521415270018622\n","-> Weighted F1 Score: 0.09911504424778764\n","-> Confusion Matrix:\n"," [[196.]]\n","ASM-6: Run model with test data and write to output file.\n","+------------------+--------+--------------+\n","|origin-folder-name|doc-name|    prediction|\n","+------------------+--------+--------------+\n","|       alt.atheism|   53658|comp.windows.x|\n","|       alt.atheism|   53660|comp.windows.x|\n","|       alt.atheism|   53661|comp.windows.x|\n","|       alt.atheism|   53663|comp.windows.x|\n","|       alt.atheism|   53664|comp.windows.x|\n","|       alt.atheism|   53667|comp.windows.x|\n","|       alt.atheism|   53669|comp.windows.x|\n","|       alt.atheism|   53670|comp.windows.x|\n","|       alt.atheism|   53671|comp.windows.x|\n","|       alt.atheism|   53673|comp.windows.x|\n","|       alt.atheism|   53675|comp.windows.x|\n","|       alt.atheism|   53676|comp.windows.x|\n","|       alt.atheism|   53677|comp.windows.x|\n","|       alt.atheism|   53753|comp.windows.x|\n","|       alt.atheism|   53754|comp.windows.x|\n","|       alt.atheism|   53755|comp.windows.x|\n","|       alt.atheism|   53756|comp.windows.x|\n","|       alt.atheism|   53757|comp.windows.x|\n","|       alt.atheism|   53758|comp.windows.x|\n","|       alt.atheism|   53759|comp.windows.x|\n","+------------------+--------+--------------+\n","only showing top 20 rows\n","\n"]}]}]}